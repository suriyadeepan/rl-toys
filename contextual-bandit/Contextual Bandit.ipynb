{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandit\n",
    "[reference](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c)\n",
    "\n",
    "[ *bandit problem*, *stateful without transition* ]\n",
    "\n",
    "![](block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we have multiple bandits with multiple arms. Which bandit's arm are we pulling? That is the context or **state** in RL terms. This state is given as input to our neural network which produces an action based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContextualBandit(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def sampleBandit(self): # return the index of a random state\n",
    "        self.state = np.random.randint(self.num_bandits)\n",
    "        return self.state\n",
    "    \n",
    "    def stateOneHot(self):\n",
    "        one_hot_state = np.zeros(self.num_bandits)\n",
    "        one_hot_state[self.state] = 1\n",
    "        return one_hot_state\n",
    "        \n",
    "    def pullArm(self, action):\n",
    "        return 1 if self.bandits[self.state][action] < np.random.randn(1) else -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, lr, num_bandits, num_actions):\n",
    "        self.state = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        W = tf.Variable(tf.zeros([num_bandits, num_actions]))\n",
    "        b = tf.Variable(tf.zeros([num_actions]))\n",
    "        y = tf.nn.softmax(tf.nn.sigmoid(tf.gather(W,self.state) + b))\n",
    "        # flatten\n",
    "        y = tf.reshape(y, [-1])\n",
    "        \n",
    "        # action\n",
    "        self.action = tf.argmax(y,0)\n",
    "        \n",
    "        # train function\n",
    "        self.reward_ = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_ = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        credit_y = tf.slice(y, self.action_, [1])\n",
    "        loss = -(tf.log(credit_y)*self.reward_)\n",
    "        self.train_fn = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# params\n",
    "tf.reset_default_graph()\n",
    "\n",
    "cbandit = ContextualBandit()\n",
    "agent = Agent(lr=0.001, num_bandits=cbandit.num_bandits, num_actions=cbandit.num_actions)\n",
    "num_epi = 10000\n",
    "rall = np.zeros(cbandit.bandits.shape)\n",
    "e = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward status : [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "Reward status : [[  -5.    5.   -7.  192.]\n",
      " [  -3.   42.  -33.  -14.]\n",
      " [ 198.  -41.  -53.  -44.]]\n",
      "Reward status : [[  -7.   10.  -12.  400.]\n",
      " [ -20.   91.  -72.  -25.]\n",
      " [ 390.  -83.  -99.  -92.]]\n",
      "Reward status : [[ -13.   10.    4.  601.]\n",
      " [ -29.  135. -107.  -29.]\n",
      " [ 584. -136. -129. -130.]]\n",
      "Reward status : [[ -21.    1.   -1.  782.]\n",
      " [ -33.  192. -127.  -24.]\n",
      " [ 794. -176. -167. -179.]]\n",
      "Reward status : [[ -35.   -6.    0.  975.]\n",
      " [ -64.  233. -166.  -26.]\n",
      " [ 994. -216. -207. -229.]]\n",
      "Reward status : [[  -43.     2.     6.  1184.]\n",
      " [  -77.   284.  -191.   -48.]\n",
      " [ 1191.  -258.  -250.  -273.]]\n",
      "Reward status : [[  -53.    -8.    16.  1386.]\n",
      " [ -124.   342.  -212.   -44.]\n",
      " [ 1382.  -304.  -283.  -331.]]\n",
      "Reward status : [[  -70.   -11.    15.  1595.]\n",
      " [ -130.   395.  -247.   -51.]\n",
      " [ 1556.  -338.  -320.  -381.]]\n",
      "Reward status : [[  -73.    -6.    22.  1788.]\n",
      " [ -152.   433.  -281.   -53.]\n",
      " [ 1732.  -382.  -367.  -428.]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    # init session\n",
    "    sess.run(init)\n",
    "    for i in range(num_epi):\n",
    "        # get state\n",
    "        state_v = cbandit.sampleBandit()\n",
    "        # choose an action\n",
    "        if np.random.randn(1) > e:\n",
    "            action_v = sess.run(agent.action, feed_dict = {agent.state : [state_v]})\n",
    "        else:\n",
    "            action_v = np.random.randint(cbandit.num_actions)\n",
    "        # let us see what the reward is, for the chosen action\n",
    "        reward_v = cbandit.pullArm(action_v)\n",
    "        # update the neural network (agent) based on the reward and\n",
    "        #  the aciton that led to it\n",
    "        sess.run(agent.train_fn, feed_dict = {\n",
    "                agent.state : [state_v],\n",
    "                agent.reward_ : [reward_v],\n",
    "                agent.action_ : [action_v]\n",
    "            })\n",
    "        # accumulate reward\n",
    "        rall[cbandit.state][action_v] += reward_v\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('Reward status : {}'.format(rall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best action for bandit #1 is action #4\n",
      "The best action for bandit #2 is action #2\n",
      "The best action for bandit #3 is action #1\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(rall):\n",
    "    print('The best action for bandit #{0} is action #{1}'.format(i+1, np.argmax(item) + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
